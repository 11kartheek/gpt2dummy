{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg09HzHrVyd6dipXFxqY2U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/11kartheek/gpt2dummy/blob/main/infer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcSCWP1FGV8x",
        "outputId": "9f4ba819-79e4-47e7-add1-e5a3b415c094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 's21-gpt3'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 19 (delta 4), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (19/19), 434.03 KiB | 4.13 MiB/s, done.\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "\n",
            "Exiting because of \"interrupt\" signal.\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/spaces/Kartheekb7/s21-gpt3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/s21-gpt3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3UmJI1CGaDS",
        "outputId": "682bcf51-ecf0-4df2-a426-6fc2f3585475"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/s21-gpt3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "id": "Nd_DQ2DYHR3i",
        "outputId": "27978044-f0e8-491c-e8dd-d17a807dfb69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "from model import *\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "model = torch.load('model.pt',map_location='cpu')\n",
        "\n",
        "\n",
        "def response(message = \"Hello, I'm a language model\", num_return_sequences = 5,max_length = 30,top_k = 50):\n",
        "  tokens = enc.encode(message)\n",
        "  tokens = torch.tensor(tokens, dtype= torch.long) # (8,) #check tiktoken app\n",
        "  tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)\n",
        "  x = tokens.to('cpu')\n",
        "  torch.manual_seed(42)\n",
        "  torch.cuda.manual_seed(42)\n",
        "  while x.size(1) < max_length:\n",
        "      # forward the model to get the logits\n",
        "      with torch.no_grad():\n",
        "          logits = model(x)[0] # (B, T, vocab_size)\n",
        "          # take the logits at the last position\n",
        "          logits = logits[:, -1, :] # (B, vocab_size)\n",
        "          # get the probabilities\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          # do top-k sampling of 50 (huggingface pipeline default)\n",
        "          # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "          topk_probs, topk_indices = torch.topk(probs, top_k, dim=-1)\n",
        "          # select a token from the top-k probabilities\n",
        "          # note: multinomial does not demand the input to sum to 1\n",
        "          ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "          # gather the corresponding indices\n",
        "          xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "          # append to the sequence\n",
        "          x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "  # print the generated text\n",
        "  return_text = \"\"\n",
        "  for i in range(num_return_sequences):\n",
        "      tokens = x[i, :max_length].tolist()\n",
        "      decoded = enc.decode(tokens)\n",
        "      return_text = return_text + \">\"+ decoded +\"\\n\"\n",
        "  return return_text"
      ],
      "metadata": {
        "id": "y2F1mygSHWfk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response(message = \"Hello, I'm a language model\", num_return_sequences = 5,max_length = 30,top_k = 50)"
      ],
      "metadata": {
        "id": "ey7NsGPum_2i",
        "outputId": "56b46f7e-a69c-4d67-ba10-2175bed7c256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\">Hello, I'm a language models all\\nThen never take truthelit since law for it rather hearal\\nShAUA: go't\\n>Hello, I'm a language models all\\nWhat but say our woman was power that have kill his\\nWhere thinks well like oneathe my power\\n>Hello, I'm a language model or drunk:\\nThat possess it might be taneily art but we play\\nAnd never lound art a\\n>Hello, I'm a language models all\\nUnless heark as I:\\nYet ere he thinks my part of traark know she did\\n>Hello, I'm a language models life\\nWhat news'T' speak an that do be brought it know he hath in we burn once I\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "mKSCKu6mMmfH",
        "outputId": "d96d4a40-e4f0-4997-c485-fa9b42a0cb70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "model = torch.load('model.pt',map_location='cpu')\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(prompt, top_k, max_return_sequences, max_tokens):\n",
        "    ans = response(message = prompt, num_return_sequences = max_return_sequences,max_length = max_tokens,top_k = top_k)\n",
        "\n",
        "\n",
        "    return ans\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=5, label=\"Input Text\"),\n",
        "        gr.Slider(minimum=1, maximum=100, value=50, step=1, label=\"Top-k\"),\n",
        "        gr.Slider(minimum=1, maximum=5, value=1, step=1, label=\"Return Sequences\"),\n",
        "        gr.Slider(minimum=10, maximum=150, value=20, step=5, label=\"Max Tokens\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Text\"),\n",
        "    title=\"GPT-2 Text Generator\",\n",
        "    description=\"Generate text using GPT-2 model with adjustable parameters.\",\n",
        "    examples=[[\"Hello, I'm a language model\"]],\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "q01w-KkCidaX",
        "outputId": "4458c773-e0d6-4990-9b11-4af51fd99d46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://3bbd5dd7196436d398.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3bbd5dd7196436d398.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load the GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(prompt, top_p, top_k, max_return_sequences, max_tokens):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        do_sample=True,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        max_length=max_tokens,\n",
        "        num_return_sequences=max_return_sequences\n",
        "    )\n",
        "\n",
        "    # Decode generated sequences\n",
        "    generated_texts = [tokenizer.decode(seq, skip_special_tokens=True) for seq in output_sequences]\n",
        "\n",
        "    return generated_texts\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=5, label=\"Input Text\"),\n",
        "        gr.Slider(minimum=0.0, maximum=1.0, default=0.9, step=0.01, label=\"Top-p (nucleus sampling)\"),\n",
        "        gr.Slider(minimum=1, maximum=100, default=50, step=1, label=\"Top-k\"),\n",
        "        gr.Slider(minimum=1, maximum=10, default=1, step=1, label=\"Max Return Sequences\"),\n",
        "        gr.Slider(minimum=10, maximum=1000, default=200, step=10, label=\"Max Tokens\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Text\"),\n",
        "    title=\"GPT-2 Text Generator\",\n",
        "    description=\"Generate text using GPT-2 model with adjustable parameters.\",\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "BFZSBRCAkbOZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}